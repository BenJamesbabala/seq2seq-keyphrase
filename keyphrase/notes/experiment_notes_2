Basic Model Specs
    bidirectional GRU
    attention
    no copy
    encdec: total number of the parameters of the model: 70,389,000
    config['enc_embedd_dim']  = 150
    config['enc_hidden_dim']  = 300

Data Preprocess
1. keep all the digits, sentence boundary detector
    sentence boundary detection, seperate with <eos>
    keep all the main punctuations
        text = re.sub('[_<>,()\.\']', ' \g<0> ', text) # pad space arround these symbols
        text = filter(lambda w: len(w) > 0, re.split('[^a-zA-Z0-9_<>,()\.\']', text)) # tokenize by non-letters
    All+Inspec - one2one
        Train pairs: 578015
        Test pairs:  2000
        Dict size:   330694
            full-digit words(^\d+\t) = 9,516, and 1,274 in first 50k words.
        Finish processing and dumping: 713 seconds
2. replace digits with <digit> and %, didn't parse keyphrase, sentence boundary detector
    All+Inspec - one2one
        Train pairs: 578015
        Test pairs:  2000
        Dict size:   342227
        Finish processing and dumping: 1209 seconds
3. keep most punctuations, replace digits with <digit>, sentence boundary detector
    Train pairs: 578015
    Test pairs:  2000
    Dict size:   320393
    looks good

Experiment
1. Not fully trained. Encounter NaN problem, need to add clipping
    extractive valid testing data=2000, Number of Target=15130/19276, Number of Prediction=20000, Number of Correct=4170
    Precision@10=0.208500, Recall@10=0.312865, F1-score10=0.234194
    Macro-Precision@10=0.208500, Macro-Recall@10=0.275611, Macro-F1-score10=0.237404

    Parameter
        total = 70,389,000
        count < 1/5/10 = 15233415 / 1468437 / 287763
        max = 18.585939

    Data: dataset/keyphrase/eos-punctuation-no_validation/

2. CopyNet
    clip_norm=2,1,0.1,0.01...
    lr=1e-3,1e-4,1e-5
    number of parameters: 78,835,750
    cannot continue after epoch=2,batch=20000
    maybe batchnorm will help
    as this version doesn't rule out the testing data, will train again

    Testing on Inspec:
    Prediction Directory:copynet-keyphrase-all.one2one.copy/predict.20161230-145652
    Data: dataset/keyphrase/eos-punctuation-no_validation/

    [1] Prediction
    path: predict.20161230-145652
    params:
        config['max_len']         = 6
        config['sample_beam']     = 1000 # for extractive prediction
        config['predict_type']    = 'extractive' # type of prediction, extractive or generative

    1. didn't filter non-appear phrases
        Log: experiments.copynet-keyphrase-all.one2one.copy.id=20161230-201623.log
        Precision@5=0.234900, Recall@5=0.196924, F1-score5=0.196583
        Macro-Precision@5=0.234900, Macro-Recall@5=0.168194, Macro-F1-score5=0.196028

        Precision@10=0.168050, Recall@10=0.270445, F1-score10=0.191894
        Macro-Precision@10=0.168050, Macro-Recall@10=0.240656, Macro-F1-score10=0.197904

        Precision@15=0.134467, Recall@15=0.318686, F1-score15=0.176841
        Macro-Precision@15=0.134467, Macro-Recall@15=0.288844, Macro-F1-score15=0.183505

    2. filter non-appear phrases
        Log: experiments.copynet-keyphrase-all.one2one.copy.id=20161230-202617.log
        Precision@5=0.313500, Recall@5=0.267870, F1-score5=0.264602
        Macro-Precision@5=0.313500, Macro-Recall@5=0.224474, Macro-F1-score5=0.261621

        Precision@10=0.244350, Recall@10=0.387334, F1-score10=0.278088
        Macro-Precision@10=0.244350, Macro-Recall@10=0.349921, Macro-F1-score10=0.287758

        Precision@15=0.196467, Recall@15=0.446257, F1-score15=0.255835
        Macro-Precision@15=0.196467, Macro-Recall@15=0.422025, Macro-F1-score15=0.268116

    3. filter partial phrases, if one phrase is part of another phrase, discard it
        Log: experiments.copynet-keyphrase-all.one2one.copy.id=20161230-230330.log
        Overall - extractive valid testing data=2000, Number of Target=13966/19275, Number of Prediction=10000, Number of Correct=3523
        Micro:		P@5=0.352300, R@5=0.264033, F1@5=0.281552
        Macro:		P@5=0.352300, R@5=0.252255, F1@5=0.294000
        --------------------------------------------------
        Overall - extractive valid testing data=2000, Number of Target=13966/19275, Number of Prediction=20000, Number of Correct=4927
        Micro:		P@10=0.246350, R@10=0.342691, F1@10=0.268914
        Macro:		P@10=0.246350, R@10=0.352785, F1@10=0.290114
        --------------------------------------------------
        Overall - extractive valid testing data=2000, Number of Target=13966/19275, Number of Prediction=30000, Number of Correct=5293
        Micro:		P@15=0.176433, R@15=0.359366, F1@15=0.223330
        Macro:		P@15=0.176433, R@15=0.378992, F1@15=0.240777

    4. filter partial phrases, keep len(phrase)=1 (affects a lot), keep phrases starts/ends with stopword (not very effective)
        Log: experiments.copynet-keyphrase-all.one2one.copy.id=20161231-004846.log
        --------------------------------------------------
        Overall - extractive valid testing data=2000, Number of Target=13966/19275, Number of Prediction=10000, Number of Correct=2744
        Micro:		P@5=0.274400, R@5=0.214355, F1@5=0.222350
        Macro:		P@5=0.274400, R@5=0.196477, F1@5=0.228991
        --------------------------------------------------
        Overall - extractive valid testing data=2000, Number of Target=13966/19275, Number of Prediction=20000, Number of Correct=4464
        Micro:		P@10=0.223200, R@10=0.334621, F1@10=0.249579
        Macro:		P@10=0.223200, R@10=0.319633, F1@10=0.262851
        --------------------------------------------------
        Overall - extractive valid testing data=2000, Number of Target=13966/19275, Number of Prediction=30000, Number of Correct=5656
        Micro:		P@15=0.188533, R@15=0.411198, F1@15=0.243214
        Macro:		P@15=0.188533, R@15=0.404984, F1@15=0.257290

        But there're some very wired single letter predictions, like the following ones:
        I guess there must be some errors in training data, in which testing data is presented as letters
            [SOURCE]: decisions , decisions , decisions a tale of special collections in the small <eos> a case study of a special collections department in a small academic library and how its collections have been acquired and developed over the years is described . <eos> it looks at the changes that have occurred in the academic environment and what effect , if any , these changes may have had on the department and how it has adapted to them . <eos> it raises questions about development and acquisitions policies and procedures
            [TARGET]: 4/6 targets
		                special collect; small academ librari; acquisit polici; case studi;
            [DECODE]: 6/182 predictions
                        [6.356]s p e c i a l
                        [7.523]c o l l e c t
                        [7.596]a c a d e m
    5. filter punctuations, <eos>. Decrease! How come? the '<' and '>' should keep
        Log: experiments.copynet-keyphrase-all.one2one.copy.id=20161231-022835.log
        --------------------------------------------------
        Overall - extractive valid testing data=2000, Number of Target=13966/19275, Number of Prediction=10000, Number of Correct=3514
        Micro:		P@5=0.351400, R@5=0.261884, F1@5=0.280160
        Macro:		P@5=0.351400, R@5=0.251611, F1@5=0.293249
        --------------------------------------------------
        Overall - extractive valid testing data=2000, Number of Target=13966/19275, Number of Prediction=20000, Number of Correct=4919
        Micro:		P@10=0.245950, R@10=0.340198, F1@10=0.267982
        Macro:		P@10=0.245950, R@10=0.352213, F1@10=0.289643
        --------------------------------------------------
        Overall - extractive valid testing data=2000, Number of Target=13966/19275, Number of Prediction=30000, Number of Correct=5279
        Micro:		P@15=0.175967, R@15=0.356629, F1@15=0.222479
        Macro:		P@15=0.175967, R@15=0.377989, F1@15=0.240140

    6. filter punctuations, <eos>. keep '<' and '>' to avoid filtering phrases contain <digit>, kind of effective predictions
        What?! Still slightly decreased!
        Log: experiments.copynet-keyphrase-all.one2one.copy.id=20161231-024257.log
        INSPEC
        --------------------------------------------------
        Overall - extractive valid testing data=2000, Number of Target=13966/19275, Number of Prediction=10000, Number of Correct=3519
        Micro:		P@5=0.351900, R@5=0.263260, F1@5=0.280939
        Macro:		P@5=0.351900, R@5=0.251969, F1@5=0.293666
        --------------------------------------------------
        Overall - extractive valid testing data=2000, Number of Target=13966/19275, Number of Prediction=20000, Number of Correct=4926
        Micro:		P@10=0.246300, R@10=0.341848, F1@10=0.268604
        Macro:		P@10=0.246300, R@10=0.352714, F1@10=0.290055
        --------------------------------------------------
        Overall - extractive valid testing data=2000, Number of Target=13966/19275, Number of Prediction=30000, Number of Correct=5293
        Micro:		P@15=0.176433, R@15=0.358596, F1@15=0.223179
        Macro:		P@15=0.176433, R@15=0.378992, F1@15=0.240777

    [2] Prediction
    change the config['sample_beam'] from 100 to 200, results improved.
    But I find that the evaluation is not quite correct. I filtered out some bad documents
        # omit the bad data which contains 0 predictions
        real_test_size = sum([1 if m['target_number'] > 0 else 0 for m in micro_metrics])

    path: predict.20161231-152451
    params:
        config['max_len']         = 6
        config['sample_beam']     = 200 # for extractive prediction
        config['predict_type']    = 'extractive' # type of prediction, extractive or generative

    Log:
        experiments.copynet-keyphrase-all.one2one.copy.id=20161231-152451.log
        experiments.copynet-keyphrase-all.one2one.copy.id=20170104-230917.log
    INSPEC:
        Overall - extractive valid testing data=2000, Number of Target=13966/19275, Number of Prediction=10000, Number of Correct=3628
        Micro:		P@5=0.362800, R@5=0.272123, F1@5=0.289624
        Macro:		P@5=0.362800, R@5=0.259774, F1@5=0.302762
        			Bpref@5=0.645500, MRR@5=0.608767
        Overall - extractive valid testing data=2000, Number of Target=13966/19275, Number of Prediction=20000, Number of Correct=5214
        Micro:		P@10=0.260700, R@10=0.362174, F1@10=0.284152
        Macro:		P@10=0.260700, R@10=0.373335, F1@10=0.307013
                    Bpref@10=0.684627, MRR@10=0.612833
        Overall - extractive valid testing data=2000, Number of Target=13966/19275, Number of Prediction=30000, Number of Correct=5723
        Micro:		P@15=0.190767, R@15=0.386270, F1@15=0.240893
        Macro:		P@15=0.190767, R@15=0.409781, F1@15=0.260338
        			Bpref@15=0.714543, MRR@15=0.613083
    NUS:
        Overall - extractive valid testing data=211, Number of Target=1660/2461, Number of Prediction=1055, Number of Correct=187
        Micro:		P@5=0.177251, R@5=0.130969, F1@5=0.135707
        Macro:		P@5=0.177251, R@5=0.112651, F1@5=0.137753
        			Bpref@5=0.437441, MRR@5=0.385703
        Overall - extractive valid testing data=211, Number of Target=1660/2461, Number of Prediction=2110, Number of Correct=273
        Micro:		P@10=0.129384, R@10=0.192779, F1@10=0.140172
        Macro:		P@10=0.129384, R@10=0.164458, F1@10=0.144828
        			Bpref@10=0.502986, MRR@10=0.398462
        Overall - extractive valid testing data=211, Number of Target=1660/2461, Number of Prediction=3165, Number of Correct=308
        Micro:		P@15=0.097314, R@15=0.212220, F1@15=0.122306
        Macro:		P@15=0.097314, R@15=0.185542, F1@15=0.127668
        			Bpref@15=0.547230, MRR@15=0.399819
    SemEval:
        Overall - extractive valid testing data=184, Number of Target=2016/2834, Number of Prediction=920, Number of Correct=214
        Micro:		P@5=0.232609, R@5=0.113474, F1@5=0.148699
        Macro:		P@5=0.232609, R@5=0.106151, F1@5=0.145777
        			Bpref@5=0.519656, MRR@5=0.448913
        Overall - extractive valid testing data=184, Number of Target=2016/2834, Number of Prediction=1840, Number of Correct=321
        Micro:		P@10=0.174457, R@10=0.171930, F1@10=0.168774
        Macro:		P@10=0.174457, R@10=0.159226, F1@10=0.166494
        			Bpref@10=0.622020, MRR@10=0.469188
        Overall - extractive valid testing data=184, Number of Target=2016/2834, Number of Prediction=2760, Number of Correct=367
        Micro:		P@15=0.132971, R@15=0.195953, F1@15=0.154515
        Macro:		P@15=0.132971, R@15=0.182044, F1@15=0.153685
        			Bpref@15=0.654293, MRR@15=0.469188

    [3] Prediction, results get worse
    change the config['sample_beam'] from 100 to voc_size(50000)
    path: copynet-keyphrase-all.one2one.copy/predict.20170104-034205.len=8.beam=voc_size
    params:
        config['max_len']         = 8
        config['sample_beam']     = voc_size
        config['predict_type']    = 'extractive' # type of prediction, extractive or generative
    INSPEC:
        Overall - extractive valid testing data=2000, Number of Target=13966/19275, Number of Prediction=10000, Number of Correct=1881
        Micro:		P@5=0.188100, R@5=0.125876, F1@5=0.140054
        Macro:		P@5=0.188100, R@5=0.134684, F1@5=0.156972
        			Bpref@5=0.460900, MRR@5=0.427975
        Overall - extractive valid testing data=2000, Number of Target=13966/19275, Number of Prediction=20000, Number of Correct=2286
        Micro:		P@10=0.114300, R@10=0.146992, F1@10=0.119830
        Macro:		P@10=0.114300, R@10=0.163683, F1@10=0.134605
        			Bpref@10=0.499803, MRR@10=0.431790
        Overall - extractive valid testing data=2000, Number of Target=13966/19275, Number of Prediction=30000, Number of Correct=2389
        Micro:		P@15=0.079633, R@15=0.151429, F1@15=0.097869
        Macro:		P@15=0.079633, R@15=0.171058, F1@15=0.108675
        			Bpref@15=0.521560, MRR@15=0.432001
    NUS:
        Overall - extractive valid testing data=211, Number of Target=1660/2461, Number of Prediction=1055, Number of Correct=73
        Micro:		P@5=0.069194, R@5=0.041593, F1@5=0.048531
        Macro:		P@5=0.069194, R@5=0.043976, F1@5=0.053775
        			Bpref@5=0.184518, MRR@5=0.139336
        Overall - extractive valid testing data=211, Number of Target=1660/2461, Number of Prediction=2110, Number of Correct=110
        Micro:		P@10=0.052133, R@10=0.063652, F1@10=0.053526
        Macro:		P@10=0.052133, R@10=0.066265, F1@10=0.058355
        			Bpref@10=0.261848, MRR@10=0.153519
        Overall - extractive valid testing data=211, Number of Target=1660/2461, Number of Prediction=3165, Number of Correct=144
        Micro:		P@15=0.045498, R@15=0.082897, F1@15=0.054857
        Macro:		P@15=0.045498, R@15=0.086747, F1@15=0.059689
        			Bpref@15=0.313060, MRR@15=0.160376
    SemEval:
        Overall - extractive valid testing data=184, Number of Target=2016/2834, Number of Prediction=920, Number of Correct=110
        Micro:		P@5=0.119565, R@5=0.053262, F1@5=0.072383
        Macro:		P@5=0.119565, R@5=0.054563, F1@5=0.074932
        			Bpref@5=0.325000, MRR@5=0.280344
        Overall - extractive valid testing data=184, Number of Target=2016/2834, Number of Prediction=1840, Number of Correct=191
        Micro:		P@10=0.103804, R@10=0.094743, F1@10=0.096602
        Macro:		P@10=0.103804, R@10=0.094742, F1@10=0.099066
        			Bpref@10=0.425906, MRR@10=0.308491
        Overall - extractive valid testing data=184, Number of Target=2016/2834, Number of Prediction=2760, Number of Correct=243
        Micro:		P@15=0.088043, R@15=0.119614, F1@15=0.098984
        Macro:		P@15=0.088043, R@15=0.120536, F1@15=0.101759
        			Bpref@15=0.490519, MRR@15=0.314062

    7. filter partial phrases, but if it appears very often, keep it (times>2)


3. Similar to 1st experiment, train a naive rnn model (no copynet)
        But training data doesn't include testing data, 1000 validation data is set to implement early stopping
        #(parameter)=70,389,000
        Initial parameter:
            - clipping=0.1
            - lr = 1e-4
            - dropout=0.5
        Testing data: inspec, nus, semeval
        Dataset statistics:
            Train pairs      : 571267
            Validation pairs : 1000
            Test pairs       : 2395
            Dict size        : 320393


    Data: dataset/keyphrase/eos-punctuation-1000validation/

    Validation Test:
        1. epoch=1.batch=5000
                ll=13.169633, 	 ppl=73.433609
        2. epoch=2.batch=3000
                ll=12.212568, 	 ppl=55.061333
        3. epoch=2.batch=5000
                ll=11.827076, 	 ppl=48.919415
        4. epoch=3.batch=1000
                ll=11.594496, 	 ppl=45.634624
    Seems not fully converged.

    path: keyphrase-all.one2one.nocopy/predict.20170103-015013
    params:
        config['max_len']         = 8
        config['sample_beam']     = voc_size (50000)
        config['predict_type']    = 'extractive' # type of prediction, extractive or generative
    Log: experiments.keyphrase-all.one2one.nocopy.id=20170103-142855.log

    INSPEC
        Overall - extractive valid testing data=2000, Number of Target=13966/19275, Number of Prediction=10000, Number of Correct=2506
        Micro:		P@5=0.250600, R@5=0.184546, F1@5=0.197035
        Macro:		P@5=0.250600, R@5=0.179436, F1@5=0.209130
        --------------------------------------------------
        Overall - extractive valid testing data=2000, Number of Target=13966/19275, Number of Prediction=20000, Number of Correct=3359
        Micro:		P@10=0.167950, R@10=0.231889, F1@10=0.181947
        Macro:		P@10=0.167950, R@10=0.240513, F1@10=0.197786
        --------------------------------------------------
        Overall - extractive valid testing data=2000, Number of Target=13966/19275, Number of Prediction=30000, Number of Correct=3626
        Micro:		P@15=0.120867, R@15=0.244167, F1@15=0.152128
        Macro:		P@15=0.120867, R@15=0.259631, F1@15=0.164946
    NUS
        Overall - extractive valid testing data=211, Number of Target=1660/2461, Number of Prediction=1055, Number of Correct=104
        Micro:		P@5=0.098578, R@5=0.068597, F1@5=0.071614
        Macro:		P@5=0.098578, R@5=0.062651, F1@5=0.076611
        --------------------------------------------------
        Overall - extractive valid testing data=211, Number of Target=1660/2461, Number of Prediction=2110, Number of Correct=172
        Micro:		P@10=0.081517, R@10=0.114929, F1@10=0.084151
        Macro:		P@10=0.081517, R@10=0.103614, F1@10=0.091247
        --------------------------------------------------
        Overall - extractive valid testing data=211, Number of Target=1660/2461, Number of Prediction=3165, Number of Correct=225
        Micro:		P@15=0.071090, R@15=0.145014, F1@15=0.086542
        Macro:		P@15=0.071090, R@15=0.135542, F1@15=0.093264
    SemEval
        Overall - extractive valid testing data=184, Number of Target=2016/2834, Number of Prediction=920, Number of Correct=126
        Micro:		P@5=0.136957, R@5=0.063726, F1@5=0.084892
        Macro:		P@5=0.136957, R@5=0.062500, F1@5=0.085831
        --------------------------------------------------
        Overall - extractive valid testing data=184, Number of Target=2016/2834, Number of Prediction=1840, Number of Correct=203
        Micro:		P@10=0.110326, R@10=0.102767, F1@10=0.103518
        Macro:		P@10=0.110326, R@10=0.100694, F1@10=0.105290
        --------------------------------------------------
        Overall - extractive valid testing data=184, Number of Target=2016/2834, Number of Prediction=2760, Number of Correct=263
        Micro:		P@15=0.095290, R@15=0.133539, F1@15=0.108259
        Macro:		P@15=0.095290, R@15=0.130456, F1@15=0.110134


new projection type methods for monotone lcp with finite termination <eos> the monotone linear complementarity problem ( lcp ) . <eos> the methods are a combination of the extragradient method and the newton method , in which the active set strategy is used and only one linear system of equations with lower dimension is solved at each iteration . <eos> it is shown that under the assumption of monotonicity , these two methods are globally and linearly convergent . <eos> furthermore , under a nondegeneracy condition they have a finite termination property . <eos> finally , the methods are extended to solving the monotone affine variational inequality problem
